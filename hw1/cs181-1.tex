\documentclass{article}
\usepackage{qtree}
\begin{document}
\begin{enumerate}
\item \textbf{Decision Trees and ID3}
  \begin{enumerate}
  \item The result of splitting on $A$:
    \begin{center}
      \Tree [.3:4 1:2 2:2 ]
    \end{center}
    The associated remainder (weighted entropy):
    \[-\left(\frac17\ln\frac13+\frac27\ln\frac23+\frac27\ln\frac24+\frac27\ln\frac24\right)\approx.669\]

    And for splitting on $B$:
    \begin{center}
      \Tree [.3:4 2:3 1:1 ]
    \end{center}
    \[-\left(\frac27\ln\frac25+\frac37\ln\frac35+\frac17\ln\frac12+\frac17\ln\frac12\right)\approx.679\]

    So splitting on $A$ provides a result with a slightly lower
    remainder, and hence slightly higher information gain.

    Splitting on $A$ may be preferable because the entropy, i.e.
    information required after the split, is lower. 
    Intuitively, splitting on $A$ might be more useful because it provides a more
    even separation of the data into the true and false branches, and hence
    a shorter tree;
    splitting on $B$ might also be useful because if $B$ is true, then that 
    branch is completely decided. 

    This shows that ID3 has inductive bias for shorter (more balanced) trees. 
  \item 
    In the following tree, going down a left branch left indicates True, and right indicates False. 

    \Tree [ .A [.B F [.C F T ] ] [ .B T T ] ]
     
    In the first stage, splitting on A generates the lowest remainder.
    Then considering the other three attributes, if we have A as True, 
    then attributes B and C are tied for lowest remainder; if A is False, 
    again B and C are tied for lowest remainder. We split on B in both cases.
    
    If A is True and B is True, there is only one case, in which the classification
    is False. Otherwise, there are two cases that are exactly dependent on C.

    If A is False and B is True, there is one case, in which the classification is
    True. Otherwise, there are two cases that cannot be separated, in which case
    we arbitrarily pick True. 
 
    6 of 7 are correctly identified by the tree. 

  \item Consider the following tree, that also correctly identifies 6 out of 7.
    Again, going left indicates True, and going right indicates False.

    \Tree [ .B [ .C T F ] [ .C F T ] ]

    We see that although the ID3 algorithm is designed to greedily try to
    get the shortest / simplest tree by maximizing information gain at every
    split, it does not always end up choosing the shortest or simplest tree,
    probably due to the myopic nature of the greedy algorithm. 
 
  \end{enumerate}
\item \textbf{ID3 with Pruning}
  \begin{enumerate}
    \setcounter{enumii}2
  \item Relative ID3 performance for pruned vs unpruned versions seem to indicate
    overfitting, as the pruned ID3 does slightly better than unpruned on
    10-fold cross-validation (so that the training and testing sets are 
    disjoint). Specifically, clean unpruned had success rate 0.87; 
    clean pruned 0.89, noisy unpruned 0.78, noisy pruned 0.8. 

    However, it is interesting to note this difference (by about the same 
    amount) for both clean and 
    noisy data, which indicates that generalization to noisier data is not
    affected by pruning, which indicates that ID3 is not overfitting to 
    training data noise. 

    TODO: verify???
  \item  
    \begin{enumerate}
    \item TODO: how implementation makes use of instance weight in splitting decisions
      Our implementation uses instance weight throughout. It is implemented already to calculate 
      entropy of a split, find the majority label in a branch, evaluate correct / incorrect,
      and so on. 
      Then it uses this information ***

      However, when we're not boosting it just weights things evenly. 
 
      AdaBoost on ID3 would not work if splitting decisions were made by counting instances
      instead of summing weights, because AdaBoost is based on the idea that you update
      weights based on which data instances need better fitting. 
    \item Weighted entropy of $\{x_1,\ldots,x_n\}$ where $y_1=T$, $y_i=F$ else, $w_1=0.5$,
      and other weights are $0.5/(n-1)$: by definition
      $entropy(D)=-\sum_{y\in Y} \frac{W_y}{W} \log_2 \frac{W_y}{W}$
      and $W_y=\sum_i w_k(i) I(y_i=y)$; we have $W_{T}=0.5$, $W_{F}=0.5(n-1)^{-1}(n-1)=0.5$, $W=1$,
      and so $$entropy(D)=-\left[ \frac{1}{2}\log_2\frac{1}{2} + \frac{1}{2}\log_2\frac{1}{2} \right] = 1$$
    \end{enumerate}
    \begin{enumerate}
    \item ******TODO:WARNING:CAVEAT*****Why does boosting do so poorly?? on task compare boosting parameters*****
      Boosting appears to work consistently better than the other methods on noisy data. 
      In particular, this appears to indicate that boosting may be less sensitive to overfitting
      by increasing the preference bias. ****???****

 
    \item 
    \item 
    \item More rounds of boosting training (generally) increases accuracy in cross-validated test sets. 
      It appears that sometimes an extra iteration of training decreases accuracy a bit, but the trend is
      overall positive until the plateau, although the variation in accuracy can be pretty large 
      (i.e. boosting 4 times is the same as boosting 10 times, though the peaks in between go up). 
      TODO: add some reasons why this is true
    \end{enumerate}
  \end{enumerate}
\item 
\end{enumerate}
\end{document}
