\documentclass{article}
\usepackage{qtree}
\begin{document}
\begin{enumerate}
\item \textbf{Decision Trees and ID3}
  \begin{enumerate}
  \item The result of splitting on $A$:
    \begin{center}
      \Tree [.3:4 1:2 2:2 ]
    \end{center}
    The associated entropy:
    \[-\left(\frac17\ln\frac13+\frac27\ln\frac23+\frac27\ln\frac24+\frac27\ln\frac24\right)\approx.669\]

    And for splitting on $B$:
    \begin{center}
      \Tree [.3:4 2:3 1:1 ]
    \end{center}
    \[-\left(\frac27\ln\frac25+\frac37\ln\frac35+\frac17\ln\frac12+\frac17\ln\frac12\right)\approx.679\]

    So splitting on $A$ provides a result with a slightly lower
    entropy, and hence slightly higher information gain.

    Splitting on $A$ might be more useful because it provides a more
    even separation of the data into the true and false branches;
    splitting on $B$ might be more useful because TODO.
  \item 
  \item 
  \end{enumerate}
\item \textbf{ID3 with Pruning}
  \begin{enumerate}
    \setcounter{enumii}2
  \item  
  \item  
    \begin{enumerate}
    \item [TODO: how implementation makes use of instance weight in splitting decisions]
      AdaBoost on ID3 would not work if splitting decisions were made by counting instances
      instead of summing weights, because AdaBoost is based on the idea that you update
      weights based on which data instances need better fitting. 
    \item Weighted entropy of $\{x_1,\ldots,x_n\}$ where $y_1=T$, $y_i=F$ else, $w_1=0.5$,
      and other weights are $0.5/(n-1)$: by definition
      $entropy(D)=-\sum_{y\in Y} \frac{W_y}{W} \log_2 \frac{W_y}{W}$
      and $W_y=\sum_i w_k(i) I(y_i=y)$; we have $W_{T}=0.5$, $W_{F}=0.5(n-1)^{-1}(n-1)=0.5$, $W=1$,
      and so $$entropy(D)=-\left[ \frac{1}{2}\log_2\frac{1}{2} + \frac{1}{2}\log_2\frac{1}{2} \right] = 1$$
    \end{enumerate}
    \begin{enumerate}
    \item 
    \item 
    \item 
    \item 
    \end{enumerate}
  \end{enumerate}
\item 
\end{enumerate}
\end{document}
