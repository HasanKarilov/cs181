\documentclass{article}
\usepackage{qtree}
\usepackage{graphicx}
\usepackage[left=1.25in,right=1.25in]{geometry}
\usepackage{color}
\definecolor{Yellow}{rgb}{1,1,0}
\newcommand{\edit}[1]{\colorbox{Yellow}{#1}}
%\newcommand{\edit}[1]{}
\begin{document}

\section*{Problem 1}

Consider training a single perceptron with the perceptron activation rule.
Assume that an image is a $3\times 3$ array of pixels, with each pixel 
being on or off. For each of the following features, either present a 
perceptron that recognizes the feature or prove none exists.
\begin{enumerate}
\item \textbf{bright or dark} - at least 75\% of the pixels are on, or 
at least 75\% of the pixels are off: none exists.
We can restate the condition as if 1, 2, 3, 7, 8, or 9 of the pixels are on,
i.e. $\sum a_i<4$ or $\sum a_i>6$, where $a_1,\ldots,a_9$ are pixels with $a_i=0$ if a pixel
is off and $a_i=1$ if it is on. This is not linearly separable. 
(If however we ran the neural network on $|\left(\sum a_i\right) -5|$ it would
have been linearly separable, but that wasn't the question.)

\item \textbf{top-bright} - a larger fraction of pixels is on in the top
row than in the bottom two rows: we present one that exists. 
Let $a_1,a_2,a_3$ be the first row and
$a_4,\ldots,a_9$ be the bottom two rows. Our perceptron classifier can
be as follows: letting $a_i=0$ denote off and $a_i=1$ denote on, 
$$P(a_1,\ldots,a_9)=2(a_1+a_2+a_3)-(a_4+\ldots+a_9)$$
is positive if a larger fraction of pixels are on in the top row than
in the bottom two rows, and nonpositive otherwise. 

\item \textbf{connected} - the set of pixels that are on is connected: 
none exists. Minsky and Papert found that perceptrons can't distinguish
XOR, so that they can't even distinguish a very narrow subproblem of
connectedness, so that perceptrons cannot determine connectedness. 
\end{enumerate}

\section*{Problem 2}
Consider four different possible learning algorithms for the digits
classification problem and argue for or against each of them given that
decision trees can be continuous: 
\begin{itemize}
\item decision trees - 
 these trees would have to be massively complex. ????
 Decision trees are good when only a few features matter. ****
\item boosted decision stumps - ??????
\item perceptrons - linear separation is very limited. They can't tell 
connectedness or separation. In order for it to work we would need to
feed it higher level features ahead of time.  

\item multi-layer feed-forward neural networks - this appears most promising. 
  It is good when many features have to be taken into account simultaneously
  but independently, which applies to our problem. It also can make complex 
  decision boundaries to arbitrary precision given enough nodes in a single
  hidden layer. One downside is that if we have a working neural network,
  it would be difficult to look into it to figure out exactly how different
  factors work together, as we would be able to in decision trees. 
\end{itemize}

\section*{Problem 3}

\begin{enumerate}
\setcount{enumi}1
\item \textbf{XOR}
Include a picture of the network generated. Explain how the combination of weights
emulates the logical operations needed to express XOR. 

\edit{I don't know how to make images not float.}

The weights aren't included in the image, so we will describe them here.
Call the inputs $A_1$ and $A_2$. 
In one hidden node (call it $B_1$), its activation is the sigmoid function
of roughly $4.0 A_1-4.1 A_2-5.0$. In the other hidden node ($B_2$), its activation
is the sigmoid of $2.7A_1-2.7A_2+2.2$. The output node has activation of
sigmoid of $5.2 B_1-5.1 B_2+2.4$. 

Looking at $B_1$, if the two inputs are the same, $B_1$ outputs 
roughly 0 (after sigmoid). If $A_1$ is 1 and $A_2$ is 0, then $B_1$ outputs roughly a half;
if $A_1$ is 0 and $A_2$ is 1, it outputs roughly 0. 
$B_2$ does similarly, except flipped: if $A_1$ and $A_2$ same, it
outputs roughly 1; if $A_1$ is 1 and $A_2$ is 0, it outputs roughly 1;
if $A_1$ is 0 and $A_2$ is 1 it outputs roughly a half. 

Then we look at the output node. If $A_1$ and $A_2$ were the same, 
$B_1$ outputs roughly 0 and $B_2$ outputs roughly 1. 

\edit{This part is just wrong. I don't really know why.}

What are some reasons the networks could intermittently fail to learn XOR?
What parameters can you tune to alleviate this type of problem?



\item \textbf{Experimental Analysis}
\begin{enumerate}
\item Provide the learning rate you used. How did you select this rate?

\item For this learning rate, chart the training set and validation error
against the number of epochs from 1 to 100. 

\begin{enumerate}
\item Are we in danger of overfitting by training for too many epochs?


\item What is a good number of epochs to train for?


\item Why is it important that we use a validation set (rather than the
actual test set) to tune the number of epochs?

If we use the test set to tune the number of epochs, the tuning may 
overfit to the testing data, resulting in better reported performance. 

\end{enumerate}
\item What is the training, validation, and test performance of the
network trained with your chosen learning rate and number of epochs?

\end{enumerate}

\item \textbf{Model Selection}
\begin{enumerate}
\item Experiment with the distributed and binary encoding for both 15 and
30 hidden layers. Which encoding performs better? Is this in line with what
we know about encodings? Use the better encoding for the rest of this problem.

\item Provide the learning rates you used for training 15 and 30 hidden units.

\setcount{enumii}3

\item In three sentences or less, explain your stopping condition. 

\item For both 15 and hidden units, use your algorithm to determine when to stop.
Graph training data and validation set performance against the number of
epochs you trained for.

\item How many epochs did you use for 15 hidden units? 30?

\item What is the test performance for 15 and 30 hidden units? Does this agree
with the validation set performance you observed? Which network structure
(15, 30, 0 hidden units) would you choose based on these experiments?


\end{enumerate}


\end{enumerate}

\section*{Problem 4}



\end{document}
