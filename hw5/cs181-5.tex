\documentclass{article}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{geometry}
\usepackage{enumerate}
%\renewcommand{\labelenumi}{(\alph{enumi})}
%\renewcommand{\labelenumii}{(\roman{enumii})}
\usepackage{color}
\definecolor{Yellow}{rgb}{1,1,0}
\newcommand{\edit}[1]{\colorbox{Yellow}{#1}}


\geometry{verbose,letterpaper,tmargin=2cm,bmargin=2cm,lmargin=2cm,rmargin=2cm}

\title{CS181: MDPs and Reinforcement Learning}
\author{Danny Zhu \& Tianhui Cai}
\let\b\mathbf

\begin{document}
\maketitle
\begin{enumerate}
\item 
\begin{enumerate}
\item {Given a probability distribution and utility function, write
  equations showing how to determine the optimal action, given the
  current score.}

  Let $U(s')$ be the utility function and $P(s'|s,a)$ the probability 
  of going to $s'$ from $s$, given action $a$. 
  Then the optimal action is
  $\pi(s,U,P)=\arg\max_{a\in A} \sum U(s')P(s'|s,a)$

%  Let $A$ denote the set of places on the target to aim for. 
%  Let $a$ denote some action in $A$ and $s$ denote your current score. 
%  Then let $Q(s,a)=R(s,a)+\sum_{s'}P(s'|s,a) B(s')$
%  where $U(s,a)$ is the utility function and $B(s')$ is the
%  best score possibly obtained with $s'$ as a current score. 
%  $B(s')$ can be obtained through expectimax or value iteration. 
%  The best action to take with $s$ as a current score is 
%  $\pi^*(s)=\argmax_{a\in A} Q(s,a)$. 

\item {One of your colleagues proposes the following utility function:}
  \begin{align*}
  U(points,score)=
  \begin{cases}
  points & \text{ if } points \leq score\\
  0 & \text{ otherwise}
  \end{cases}
  \end{align*}
  {In other words, the utility is equivalent to your change in score. What
  do you think of this proposal? If a dart player is designed using this
  proposal, what decisions would you expect it to make well, and what
  decisions would you expect it to make poorly?}

  This proposal is myopic. It would work well if lower point values were at least 
  as easily obtained as larger ones. However, this may not be the case, i.e. if 
  you started out with 10 points, and you could hit 5 and 9 with high probability,
  but 1 with extremely low probability, then it is better to aim for 5 and 5 
  rather than use the proposal which would suggest 9 and 1. 
  \end{enumerate}

\item
  \begin{enumerate}
  \item {What are the states and actions of your MDP model?
  Complete the \texttt{get\_states()} function in \texttt{darts.py}. (you may use
  the \texttt{test get states} unit test at this point)}

  The states are integers from 0 to 301. The actions are aiming for a
  particular section on the dartboard.  

  \item {Define the reward function: what reward or punishment
  do you receive for each state and action? What role does the discount
  factor play? Once you have specified this function, complete the
  reward function \texttt{R()} in \texttt{darts.py}. (see also the 
  \texttt{test R} unit test)}

  Given a state $s$ and action $a$, if $s=0$, reward is 1; if $s\neq 0$,
  reward is 0, since we win when we have a score of 0. This does not
  depend on action. 
  (I think it should be 0 vs -1 for rewards, but the tests say that's wrong.)

  If discount factor is 1, we weight all time periods equally. This is
  not so good because it would mean we didn't care whether we won early
  or late. If discount factor is low, it means that we'd much rather win
  now than later, i.e. earlier winning is better than later. 
  If discount factor is 0, it means we don't care at all about future
  winning, which is bad, since it only acts nontrivially if we are one
  step away from winning. 

  \setcounter{enumii}3
  \item {We have implemented the infinite horizon value iteration
  algorithm in \texttt{mdp.py}. Why might we choose to use an infinite rather
  than finite horizon to find an optimal policy in the dartboards scenario? 
  You may now get your first results by running the \texttt{Warm-up} task.}

  There may be infinite steps, i.e. say we have one point left to go
  and we keep not getting 1 point. 
  Also, discounting is a good thing, and infinite horizon does discounting.  
  
  \item {Describe the intuition behind an optimal policy resulting
  from value iteration using the small game.}

  \item {Still referring to the small game, how does the optimal
  policy vary as you change the discount factor? What is common
  across optimal policies?}



  \end{enumerate}

\item 
  \begin{enumerate}
  \item {Prove that if policy iteration terminates with the policy
  $\pi^*$, $\pi^*$ is an optimal stationary policy.}

  Policy iteration consists of repeating the following: given a $\pi(s)$,
  convert it to a set of $V(s)$ that agrees with it, then take the
  argmax of $Q(s,a)=R(s,a)+\gamma \sum_{s'} P(s'|s,a)V(s')$. This step
  is basically the same as one step of value iteration, which converges
  to a unique fixed point, i.e. between different steps, $V$ necessarily
  changes unless we have reached the maximum. 

  So if value iteration has $\pi^*$ as a 
  stationary point, i.e. if we take the corresponding $V$, and through 
  the step $\pi$ and hence $V$ doesn't change, then it must be optimal
  by convergence / optimality of value iteration, i.e. if some $V$ is 
  not optimal, on one step of value iteration $V$ necessarily will change,
  so that $\pi$ necessarily changes on one step of policy iteration. 

  \item {Prove that if $\pi^1$ is the policy before a policy iteration
  phase, and $\pi^2$ is the policy after the phase, then for every state $s$,
  $V^{\pi^2}(s)\geq V^{\pi^1}(s)$. [Hint: first try to prove $V^{\pi^2}(\hat s)\geq V^{\pi^1}(\hat s)$
  when the action is only improved in state $\hat s$. Then argue this property
  holds for all states.]}

  Assume not, i.e. that $V^{\pi^2}(\hat s)<V^{\pi^1}(\hat s)$ for some $s$.
  Given $\pi_1$, by definition $V_1:=V^{\pi^2}$ is such that
  $V_1(s)=R(s,\pi_1(s))+\gamma \sum_{s'}P(s'|s,\pi_1(s))V_1(s')$.
  By definition of $V_2:=V^{\pi^2}$, $V_2(s)=\max_a \left[R(s,a)+\gamma \sum_{s'}R(s'|s,a)V_1(s)\right]$.
  Then the claim is that (for contradiction)
  $$\max_a \left[R(s,a)+\gamma \sum_{s'}R(s'|s,a)V_1(s)\right] < R(s,\pi_1(s))+\gamma \sum_{s'}P(s'|s,\pi_1(s))V_1(s')$$
  But this is impossible, because for $a$ we can just take $\pi(s)$ to obtain
  an equality, a contradiction. 


  \item {Deduce that policy iteration always terminates with an
  optimal stationary policy.}
  
  There are a finite number of policies, and each iteration either produces
  the same policy (in which case we're done, and by part (a) it's optimal)
  or we get a policy (approximately) strictly better (approximately being, 
  if we have a sensible ordering so we don't oscillate between policies), so
  that there are a finite number of steps to go through. 

  \end{enumerate}

\item 
  \begin{enumerate}
  \item {Implement two exploration / exploitation strategies for deciding how to act
  in each epoch based on the current result of value iterations: these should be different.
  Run the next tasks and create a graph of relationship between performance and epoch
  size. How do these graphs compare for both strategies?}

  \item {Implement the model-free $Q$ learning algorithm as well as two exploration/
  exploitation strategies that may be the same asin part (a). Pick a learning rate. Write 
  code. Use the \texttt{Q-learning} task with the medium size game to compare the 
  performance of your algorithm for both exploration / exploitation strategies.}

  \item {Compare the performance of the model-based and model-free algorithms. Why 
  do you think these algorithms performed relatively well or poorly?}


  \end{enumerate}

\end{enumerate}
\end{document}
